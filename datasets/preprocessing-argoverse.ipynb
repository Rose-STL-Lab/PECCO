{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9857ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Functions loading the .pkl version preprocessed data\"\n",
    "from tensorpack import dataflow\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "import time\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argoverse.map_representation.map_api import ArgoverseMap\n",
    "from argoverse.data_loading.argoverse_forecasting_loader import ArgoverseForecastingLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78649a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseTest(object):\n",
    "    \"\"\"\n",
    "    Data flow for argoverse dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, shuffle: bool = True, random_rotation: bool = False,\n",
    "                 max_car_num: int = 50, freq: int = 10, use_interpolate: bool = False, \n",
    "                 use_lane: bool = False, use_mask: bool = True):\n",
    "        if not os.path.exists(file_path):\n",
    "            raise Exception(\"Path does not exist.\")\n",
    "\n",
    "        self.afl = ArgoverseForecastingLoader(file_path)\n",
    "        self.shuffle = shuffle\n",
    "        self.random_rotation = random_rotation\n",
    "        self.max_car_num = max_car_num\n",
    "        self.freq = freq\n",
    "        self.use_interpolate = use_interpolate\n",
    "        self.am = ArgoverseMap()\n",
    "        self.use_mask = use_mask\n",
    "        self.file_path = file_path\n",
    "        \n",
    "\n",
    "    def get_feat(self, scene):\n",
    "\n",
    "        data, city = self.afl, self.afl[scene].city\n",
    "\n",
    "        lane = np.array([[0., 0.]], dtype=np.float32)\n",
    "        lane_drct = np.array([[0., 0.]], dtype=np.float32)\n",
    "\n",
    "\n",
    "        tstmps = data.TIMESTAMP.unique()\n",
    "        tstmps.sort()\n",
    "\n",
    "        data = self._filter_imcomplete_data(data, tstmps, 50)\n",
    "\n",
    "        data = self._calc_vel(data, self.freq)\n",
    "\n",
    "        agent = data[data['OBJECT_TYPE'] == 'AGENT']['TRACK_ID'].values[0]\n",
    "\n",
    "        car_mask = np.zeros((self.max_car_num, 1), dtype=np.float32)\n",
    "        car_mask[:len(data.TRACK_ID.unique())] = 1.0\n",
    "\n",
    "        feat_dict = {'city': city, \n",
    "                     'lane': lane, \n",
    "                     'lane_norm': lane_drct, \n",
    "                     'scene_idx': scene,  \n",
    "                     'agent_id': agent, \n",
    "                     'car_mask': car_mask}\n",
    "\n",
    "        pos_enc = [subdf[['X', 'Y']].values[np.newaxis,:] \n",
    "                   for _, subdf in data[data['TIMESTAMP'].isin(tstmps[:19])].groupby('TRACK_ID')]\n",
    "        pos_enc = np.concatenate(pos_enc, axis=0)\n",
    "        # pos_enc = self._expand_dim(pos_enc)\n",
    "        feat_dict['pos_2s'] = self._expand_particle(pos_enc, self.max_car_num, 0)\n",
    "\n",
    "        vel_enc = [subdf[['vel_x', 'vel_y']].values[np.newaxis,:] \n",
    "                   for _, subdf in data[data['TIMESTAMP'].isin(tstmps[:19])].groupby('TRACK_ID')]\n",
    "        vel_enc = np.concatenate(vel_enc, axis=0)\n",
    "        # vel_enc = self._expand_dim(vel_enc)\n",
    "        feat_dict['vel_2s'] = self._expand_particle(vel_enc, self.max_car_num, 0)\n",
    "\n",
    "        pos = data[data['TIMESTAMP'] == tstmps[19]][['X', 'Y']].values\n",
    "        pos = self._expand_dim(pos)\n",
    "        feat_dict['pos0'] = self._expand_particle(pos, self.max_car_num, 0)\n",
    "        vel = data[data['TIMESTAMP'] == tstmps[19]][['vel_x', 'vel_y']].values\n",
    "        vel = self._expand_dim(vel)\n",
    "        feat_dict['vel0'] = self._expand_particle(vel, self.max_car_num, 0)\n",
    "        track_id =  data[data['TIMESTAMP'] == tstmps[19]]['TRACK_ID'].values\n",
    "        feat_dict['track_id0'] = self._expand_particle(track_id, self.max_car_num, 0, 'str')\n",
    "        feat_dict['frame_id0'] = 0\n",
    "    \n",
    "    \n",
    "        for t in range(31):\n",
    "            pos = data[data['TIMESTAMP'] == tstmps[19 + t]][['X', 'Y']].values\n",
    "            pos = self._expand_dim(pos)\n",
    "            feat_dict['pos' + str(t)] = self._expand_particle(pos, self.max_car_num, 0)\n",
    "            vel = data[data['TIMESTAMP'] == tstmps[19 + t]][['vel_x', 'vel_y']].values\n",
    "            vel = self._expand_dim(vel)\n",
    "            feat_dict['vel' + str(t)] = self._expand_particle(vel, self.max_car_num, 0)\n",
    "            track_id =  data[data['TIMESTAMP'] == tstmps[19 + t]]['TRACK_ID'].values\n",
    "            feat_dict['track_id' + str(t)] = self._expand_particle(track_id, self.max_car_num, 0, 'str')\n",
    "            feat_dict['frame_id' + str(t)] = t\n",
    "        \n",
    "        return feat_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(glob(os.path.join(self.file_path, '*')))\n",
    "\n",
    "    @classmethod\n",
    "    def _expand_df(cls, data, city_name):\n",
    "        timestps = data['TIMESTAMP'].unique().tolist()\n",
    "        ids = data['TRACK_ID'].unique().tolist()\n",
    "        df = pd.DataFrame({'TIMESTAMP': timestps * len(ids)}).sort_values('TIMESTAMP')\n",
    "        df['TRACK_ID'] = ids * len(timestps)\n",
    "        df['CITY_NAME'] = city_name\n",
    "        return pd.merge(data, df, on=['TIMESTAMP', 'TRACK_ID'], how='right')\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def __calc_vel_generator(cls, df, freq=10):\n",
    "        for idx, subdf in df.groupby('TRACK_ID'):\n",
    "            sub_df = subdf.copy().sort_values('TIMESTAMP')\n",
    "            sub_df[['vel_x', 'vel_y']] = sub_df[['X', 'Y']].diff() * freq\n",
    "            yield sub_df.iloc[1:, :]\n",
    "\n",
    "    @classmethod\n",
    "    def _calc_vel(cls, df, freq=10):\n",
    "        return pd.concat(cls.__calc_vel_generator(df, freq=freq), axis=0)\n",
    "    \n",
    "    @classmethod\n",
    "    def _expand_dim(cls, ndarr, dtype=np.float32):\n",
    "        return np.insert(ndarr, 2, values=0, axis=-1).astype(dtype)\n",
    "    \n",
    "    @classmethod\n",
    "    def _linear_interpolate_generator(cls, data, col=['X', 'Y']):\n",
    "        for idx, df in data.groupby('TRACK_ID'):\n",
    "            sub_df = df.copy().sort_values('TIMESTAMP')\n",
    "            sub_df[col] = sub_df[col].interpolate(limit_direction='both')\n",
    "            yield sub_df.ffill().bfill()\n",
    "    \n",
    "    @classmethod\n",
    "    def _linear_interpolate(cls, data, col=['X', 'Y']):\n",
    "        return pd.concat(cls._linear_interpolate_generator(data, col), axis=0)\n",
    "    \n",
    "    @classmethod\n",
    "    def _filter_imcomplete_data(cls, data, tstmps, window=20):\n",
    "        complete_id = list()\n",
    "        for idx, subdf in data[data['TIMESTAMP'].isin(tstmps[:window])].groupby('TRACK_ID'):\n",
    "            if len(subdf) == window:\n",
    "                complete_id.append(idx)\n",
    "        return data[data['TRACK_ID'].isin(complete_id)]\n",
    "    \n",
    "    @classmethod\n",
    "    def _expand_particle(cls, arr, max_num, axis, value_type='int'):\n",
    "        dummy_shape = list(arr.shape)\n",
    "        dummy_shape[axis] = max_num - arr.shape[axis]\n",
    "        dummy = np.zeros(dummy_shape)\n",
    "        if value_type == 'str':\n",
    "            dummy = np.array(['dummy' + str(i) for i in range(np.product(dummy_shape))]).reshape(dummy_shape)\n",
    "        return np.concatenate([arr, dummy], axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class process_utils(object):\n",
    "            \n",
    "    @classmethod\n",
    "    def expand_dim(cls, ndarr, dtype=np.float32):\n",
    "        return np.insert(ndarr, 2, values=0, axis=-1).astype(dtype)\n",
    "    \n",
    "    @classmethod\n",
    "    def expand_particle(cls, arr, max_num, axis, value_type='int'):\n",
    "        dummy_shape = list(arr.shape)\n",
    "        dummy_shape[axis] = max_num - arr.shape[axis]\n",
    "        dummy = np.zeros(dummy_shape)\n",
    "        if value_type == 'str':\n",
    "            dummy = np.array(['dummy' + str(i) for i in range(np.product(dummy_shape))]).reshape(dummy_shape)\n",
    "        return np.concatenate([arr, dummy], axis=axis)\n",
    "    \n",
    "\n",
    "def get_max_min(datas):\n",
    "    mask = datas['car_mask']\n",
    "    slicer = mask[0].astype(bool).flatten()\n",
    "    pos_keys = ['pos0'] + ['pos_2s']\n",
    "    max_x = np.concatenate([np.max(np.stack(datas[pk])[0,slicer,...,0]\n",
    "                                   .reshape(np.stack(datas[pk]).shape[0], -1), \n",
    "                                   axis=-1)[...,np.newaxis]\n",
    "                            for pk in pos_keys], axis=-1)\n",
    "    min_x = np.concatenate([np.min(np.stack(datas[pk])[0,slicer,...,0]\n",
    "                                   .reshape(np.stack(datas[pk]).shape[0], -1), \n",
    "                                   axis=-1)[...,np.newaxis]\n",
    "                            for pk in pos_keys], axis=-1)\n",
    "    max_y = np.concatenate([np.max(np.stack(datas[pk])[0,slicer,...,1]\n",
    "                                   .reshape(np.stack(datas[pk]).shape[0], -1), \n",
    "                                   axis=-1)[...,np.newaxis]\n",
    "                            for pk in pos_keys], axis=-1)\n",
    "    min_y = np.concatenate([np.min(np.stack(datas[pk])[0,slicer,...,1]\n",
    "                                   .reshape(np.stack(datas[pk]).shape[0], -1), \n",
    "                                   axis=-1)[...,np.newaxis]\n",
    "                            for pk in pos_keys], axis=-1)\n",
    "    max_x = np.max(max_x, axis=-1) + 10\n",
    "    max_y = np.max(max_y, axis=-1) + 10\n",
    "    min_x = np.max(min_x, axis=-1) - 10\n",
    "    min_y = np.max(min_y, axis=-1) - 10\n",
    "    return min_x, max_x, min_y, max_y\n",
    "\n",
    "\n",
    "def process_func(putil, datas, am):\n",
    "    \n",
    "    city = datas['city'][0]\n",
    "    x_min, x_max, y_min, y_max = get_max_min(datas)\n",
    "\n",
    "    seq_lane_props = am.city_lane_centerlines_dict[city]\n",
    "\n",
    "    lane_centerlines = []\n",
    "    lane_directions = []\n",
    "\n",
    "    # Get lane centerlines which lie within the range of trajectories\n",
    "    for lane_id, lane_props in seq_lane_props.items():\n",
    "\n",
    "        lane_cl = lane_props.centerline\n",
    "\n",
    "        if (\n",
    "            np.min(lane_cl[:, 0]) < x_max\n",
    "            and np.min(lane_cl[:, 1]) < y_max\n",
    "            and np.max(lane_cl[:, 0]) > x_min\n",
    "            and np.max(lane_cl[:, 1]) > y_min\n",
    "        ):\n",
    "            lane_centerlines.append(lane_cl[1:])\n",
    "            lane_drct = np.diff(lane_cl, axis=0)\n",
    "            lane_directions.append(lane_drct)\n",
    "    if len(lane_centerlines) > 0:\n",
    "        lane = np.concatenate(lane_centerlines, axis=0)\n",
    "        # lane = putil.expand_dim(lane)\n",
    "        lane_drct = np.concatenate(lane_directions, axis=0)\n",
    "        # lane_drct = putil.expand_dim(lane_drct)[...,:3]\n",
    "\n",
    "        datas['lane'] = [lane]\n",
    "        datas['lane_norm'] = [lane_drct]\n",
    "        return datas\n",
    "    else:\n",
    "        return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2290555",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = ArgoverseMap()\n",
    "putil = process_utils()\n",
    "\n",
    "dataset_path = '.'\n",
    "\n",
    "val_path = os.path.join(dataset_path, 'val_original', 'data')\n",
    "train_path = os.path.join(dataset_path, 'train_original', 'data')\n",
    "\n",
    "afl_train = ArgoverseForecastingLoader(train_path)\n",
    "afl_val = ArgoverseForecastingLoader(val_path)\n",
    "at_train = ArgoverseTest(train_path, max_car_num=60)\n",
    "at_val = ArgoverseTest(val_path, max_car_num=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933434fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5663f9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"++++++++++++++++++++ START TRAIN ++++++++++++++++++++\")\n",
    "train_num = len(afl_train)\n",
    "batch_start = time.time()\n",
    "os.mkdir(os.path.join(dataset_path, 'train'), exist_ok=True)\n",
    "for i, scene in enumerate(range(train_num)):\n",
    "    if i % 100 == 0:\n",
    "        batch_end = time.time()\n",
    "        print(\"SAVED ============= {} / {} ....... {}\".format(i, train_num, batch_end - batch_start))\n",
    "        batch_start = time.time()\n",
    "\n",
    "    data = {k:[v] for k, v in at_train.get_feat(scene).items()}\n",
    "    datas = process_func(putil, data, am)\n",
    "    with open(os.path.join(dataset_path, 'train', str(datas['scene_idx'][0])+'.pkl'), 'wb') as f:\n",
    "        pickle.dump(datas, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3fb3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++ START VAL ++++++++++++++++++++\")\n",
    "val_num = len(afl_val)\n",
    "batch_start = time.time()\n",
    "for i, scene in enumerate(range(val_num)):\n",
    "    if i % 1000 == 0:\n",
    "        batch_end = time.time() \n",
    "        print(\"SAVED ============= {} / {} ....... {}\".format(i, val_num, batch_end - batch_start))\n",
    "        batch_start = time.time()\n",
    "\n",
    "    data = {k:[v] for k, v in at_val.get_feat(scene).items()}\n",
    "    datas = process_func(putil, data, am)\n",
    "    with open(os.path.join(dataset_path, 'val/', str(datas['scene_idx'][0])+'.pkl'), 'wb') as f:\n",
    "        pickle.dump(datas, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pecco",
   "language": "python",
   "name": "pecco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
